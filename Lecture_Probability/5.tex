\input{../settings}
\begin{document}

\title{Probability}

\author{QSnake Edition}

\maketitle

\subsection*{1. Joint Probability Distribution of Functions of Random Variables}$ $

\textbf{Recall}. Change of Variables in a Double integral

Suppose that $T$ is a $C^1$ transformation whose Jacobian is nonzero and that $T$ maps a region $S$ in the $uv-plane$ onto a region $R$ in the $xy$-plane. Suppose that $f$ is continuous on $R$ and that $R$ and $S$ are type I or type II plane regions. Suppose also that $T$ is one-to-one, except perhaps on the boundary of $S$. Then

$$\iint_R f(x,y)dA = \iint_S f(x(u,v),y(u,v))\left|  \dfrac{\partial(x,y)}{\partial(u,v)} \right|$$



In Probability, we change the notation here:

$X_1,X_2$ be jointly continuous random variables with joint probability density function $F_{X_1,X_2}$. Suppose that $Y_1 = g_1(X_1,X_2)$ and $Y_2 = g_2(X_1,X_2)$ for some functions $g_1$ and $g_2$

we denote Jacobian as

$$J(x_1,x_2) = \left| \begin{matrix}
	\dfrac{\partial g_1}{\partial x_1} & \dfrac{\partial g_1}{\partial x_2} \\
	\dfrac{\partial g_2}{\partial x_1} & \dfrac{\partial g_2}{\partial x_2}
\end{matrix} \right| = \dfrac{\partial g_1}{\partial x_1}\dfrac{\partial g_2}{\partial x_2} - \dfrac{\partial g_1}{\partial x_2}\dfrac{\partial g_2}{\partial x_1} \neq 0$$

\textbf{Example}(p. 279)

Let $X_1$ and $X_2$ be jointly continuous random variables with probability density function $f_{X_1,X_2}$. Let $Y_1 = X_1 + X_2,~Y_2 = X_1 - X_2$. Find the joint density function of $Y_1$ and $Y_2$ in terms of $f_{X_1,X_2}$

\newpage

\subsection*{2. Little Note in Independent Random Variable}

\begin{thm*}
	Two random variables $X_1$ and $X_2$ with joint pdf $f(x_1,x_2)$ are independent if and only if :
	
	\begin{enumerate}
		\item the 'support set', $\{(x_1,x_2)~|~ f(x_1,x_2) > 0\}$, is a Cartesian product, $A \times B$, and
		\item the joint pdf can be factored into the product of functions of $x_1$ and $x_2$, $f(x_1,x_2) = g(x_1)h(x_2)$
	\end{enumerate}
\end{thm*}

\textbf{Example 1}

The joint pdf of a pair $X_1$ and $X_2$ is 

$$f(x_1,x_2) = 8x_1x_2 \hspace{5pt} 0 < x_1 < x_2 < 1$$

and zero otherwise. This function can clearly be factored according to part (2) of the theorem, but the support set, $\{(x_1,x_2) ~|~ 0 < x_1 < x_2 < 1\}$, is a triangular region that cannot be represented as a Cartesian product. Thus, $X_1$ and $X_2$ are dependent.

$ $ 

\textbf{Example 2}

Consider now a pair $X_1$ and $X_2$ with joint pdf

$$f(x_1,x_2) = x_1 + x_2 \hspace{5pt} 0 < x_1 < 1,~0 < x_2 < 1$$

and zero otherwise. In this case the support set is $\{(x_1,x_2)~|~ 0 < x_1 < 1 \text{ and } 0 < x_1 < 1\}$, which can be represented as $A \times B$, where $A$ and $B$ are both the open interval $(0,1)$. However, part (2) of the theorem is not satisfied because $x_1 + x_2$ cannot be factored as $g(x_1)h(x_2)$. Thus $X_1$ and $X_2$ are dependent.


\newpage

\subsection*{3. Correlation}

\begin{defn}
	If $X$ and $Y$ are random variables with variances $\sigma^2_X$ and $\sigma^2_Y$ and covariance $\sigma_{XY} = Cov(X,Y)$, then the correlation coefficient of $X$ and $Y$ is 
	
	$$\rho = \dfrac{\sigma_{XY}}{\sigma_X\sigma_Y} $$
	
	The random variables $X$ and $Y$ are said to be uncorrelated if $\rho = 0$; otherwise they are said to be correlated.
\end{defn}

\begin{thm*}
	If $\rho$ is the correlation coefficient of $X$ and $Y$, then
	
	$$-1 \leq \rho \leq 1$$
	
	and $\rho = \pm 1 \text{ if and only if } Y = aX + b \text{ with probability } 1 \text{ for some} a \neq 0 \text{ and } b $
\end{thm*}

\begin{thm*}
	If $E(Y~|~x)$ is a linear function of $x$, then
	
	$$E(Y~|~x) = \mu_2 + \rho \dfrac{\sigma_2}{\sigma_1}(x - \mu_1)$$
	
	and
	
	$$E_X[Var(Y~|~X)] = \sigma_2^2(1 - \rho^2)$$
\end{thm*}

\begin{proof}
	If $E(Y~|~x) = ax + b$, then
	
	$$\mu_2 = E(Y) = E_X[E(Y~|~X)] = E_X(aX + b) = a\mu_1 + b$$
	
	and
	
	\begin{eqnarray*}
		\sigma_{XY} &=& E[(X - \mu_1)(Y - \mu_2)] \\
		&=& E[(X - \mu_1)Y] \hspace{10pt} \\
		&=& E_X\{E[(X - \mu_1)Y~|~X]\}\\
		&=& E_X[(X - \mu_1)E(Y~|~X)]\\
		&=& E_X[(X - \mu_1)(aX + b)]\\
		&=& a\sigma_1^2
	\end{eqnarray*}
	
	In second line, we use the fact:$Cov(X,Y) = E[XY] - E[X]E[Y]$
	
	in last 2 line, we use the fact:$E[g(X)Y~|~x] = g(x)E[Y~|~X]$
	
	Thus,
	
	$$a = \dfrac{\sigma_{XY}}{\sigma_1^2} = \rho \dfrac{\sigma_2}{\sigma_1} \hspace{5pt} \text{and} \hspace{5pt} b = \mu_2 - \rho \dfrac{\sigma_2}{\sigma_1}\mu_1$$
	
\end{proof}

\begin{proof}
	\begin{eqnarray*}
		E_X[Var(Y~|~X)] &=& Var(Y) - Var_X\left[ \mu_2 + \rho \dfrac{\sigma_2}{\sigma_1}(X - \mu_1) \right] \\
		&=& Var(Y) - \rho^2 \dfrac{\sigma_2^2 \sigma_1^2}{\sigma_1^2}\\
		&=& \sigma^2_2(1 - \rho^2)
	\end{eqnarray*}
\end{proof}

\newpage

\subsection*{Limit Theorem}

\begin{prop*}[Markov's inequality]

If $X$ is a random variable that takes only nonnegative values, then for any value $a>0$,

$$P\{X \geq a\} \leq \dfrac{E[X]}{a}$$
	
\end{prop*}

\begin{proof}
	For $a>0$, let
	
	$$I = \begin{cases}
		1 & $if $ X \geq a\\
		0 & $otherwise$
	\end{cases}$$
	
	and not that, since $X \geq 0$

	$$I \leq \dfrac{X}{a}$$

	Taking expectations of the preceding inequality yields

	$$E[I] \leq \dfrac{E[X]}{a}$$

	which, because $E[I] = P\{X \geq a\}$, proves the result
\end{proof}

\begin{prop*}[Chebyshev's inequality]

	If $X$ is a random variable with finite mean $\mu$ and variance $\sigma^2$, then for any value $k > 0$
	
	$$P \{|X - \mu| \geq k\} \leq \dfrac{\sigma^2}{k^2}$$
	
\end{prop*}

\begin{proof}
	Since $(X - \mu)^2$ is a nonnegative random variable, we can apply Markov's inequality (with $a = k^2$) to obtain
	
	$$P\{(X - \mu)^2 \geq k^2\} \leq \dfrac{E[(X - \mu)^2]}{k^2}$$
	
	But since $(X - \mu)^2 \geq k$ if and only if $|X - \mu| \geq k$
	
	$$P\{X - \mu \geq k\} \leq \dfrac{E[(X - \mu)^2]}{k^2} = \dfrac{\sigma^2}{k^2}$$
\end{proof}

\newpage

\begin{prop*}
	If $Var(X) = 0$, then
	
	$$P\{X = E[X]\} = 1$$
	
	In other words, the only random variables having variances equal to $0$ are those that are constant with probability $1$.
\end{prop*}

\begin{proof}
	By Chebyshev's inequality, we have, for any $n \geq 1$,
	
	$$P\left\{ |X - \mu| > \dfrac{1}{n} \right\} = 0$$
	
	Letting $n \rightarrow \infty$ and using the continuity property of probability yields
	
	$$0 = \lim_{n \rightarrow \infty}P\left\{|X - \mu| > \dfrac{1}{n} \right\} = P \left\{.\lim_{n \rightarrow \infty} \left\{|X - \mu| > \dfrac{1}{n} \right\} \right\} = P\{X \neq \mu\}$$
	
	and the result is established
\end{proof}











\end{document}